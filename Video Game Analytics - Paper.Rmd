---
title: "Final Paper"
author: "STOR 320.02 Group 16"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(dplyr)
library(leaps)
library(knitr)
library(kableExtra)
library(psych)
library(broom)
library(glmnet)

developers <- read_csv("/Users/meghanpartrick/Documents/STOR 320/developers.csv")
games <- read_csv("/Users/meghanpartrick/Documents/STOR 320/games.csv")
genres <- read_csv("/Users/meghanpartrick/Documents/STOR 320/genres.csv")

developers_and_games <- merge(developers, games, by="id", all.x=TRUE, all.y=TRUE)
backloggd <- merge(developers_and_games, genres, by="id", all.x=TRUE, all.y=TRUE)
```

# INTRODUCTION

In the constantly evolving landscape of the video gaming industry, it remains that a common measure of a game’s success is by average rating. A rating can be generated by a single user, a professional organization, or it can be obtained through software designed particularly for review. Often, ratings are measured on a discrete scale, meaning there is a finite set of possible values like 1.0, 1.5, 2.0, and so on. When averaged across thousands or millions of users or professionals, an average rating may extend beyond the given discrete scale; for example, an average rating may be 1.7 even if the possible rating values range from 0.0 to 5.0 by increments of 0.5. With this in mind, the ultimate goal of video game developers is to maximize success, and therefore average rating. The aim of this analysis is to determine predictive models given - or not given - certain information about the video game that can then predict video game average rating within a certain level of accuracy. To narrow the scope of this study, two primary questions are addressed.

First, could we predict the success of a game without knowing the publisher? It is reasonable to hypothesize that knowing the developer impacts a user’s purchasing decision, thus affecting the amount and types of ratings that follow. For example, suppose two new video games are released, both falling under the puzzle genre for young audiences. Both games are identical on every account except the first game is published by a well-known developer like Nintendo or Atari, and the second game is published by a smaller, lesser-known developer called WeLovePuzzles. Most likely, a user would be driven to purchase from a big brand like Nintendo, providing them with not only a greater profit, but more (and likely higher) reviews. Now we seek to analyze situations in which the buyer is not provided with information on the developer: which game would they choose? This drives us to search for other measures of a game’s success, and then to use those as variables in a model to predict average rating. This could benefit all types of publishers since understanding factors besides developer that contribute to success would drive businesses to focus more on the growth of these individual features rather than on the use of brand-name recognition to sell their game and achieve greater success. As a result, this would allow lesser-known publishers to catch up with these bigger brands since developer would not be a variable affecting the average rating.

Second, can we come up with creative metrics on the publisher to help us predict the rating of the game more accurately? As alluded to in the discussion of our first question, the primary way we consider developers is by how big or successful they are. But what does this mean? Rather than using developer names categorically in a predictive model, we focus on finding particular numeric measures that increase predictive accuracy. For example, perhaps standard publisher ratings or the number of games they have released in the past decade increase our ability to forecast average game rating. Or maybe the memoryless property plays a role in that the only relevant predictors of a developer’s game are those of the last released video game. All of this is to be explored in our analysis, and will follow closely from the investigation of our first question.

By answering these questions throughout our study, we pave the way for modeling video game success. With the power of accurately predictive average ratings, users will be able to get the most bang for their buck, and developers will be able to generate maximum revenue while focusing on the most influential factors affecting game success. Overall, learning to predict average rating in the presence or absence of certain metrics will result in more successful games, more satisfied customers, and more profitable development strategies.

# DATA

The data source utilized to address these questions was scraped from the Backloggd website by Simon Garanin - a datasets contributor from Russia - and uploaded publicly to Kaggle, which is where we exported the dataset as a comma separated values (CSV) file. The game metadata itself that is available on Backloggd comes from a community database called the Internet Game Database (IGDB). The Backloggd website is an online hub to virtually track personal video game collections. It provides a one-stop platform where users can rate games they have played, grow their wishlist, and continue to update their backlog (a list of games the user plans to play next). Backloggd also has a social component, where it’s possible to connect with your friends to see what their playing behaviors and ratings are instantly. The creators of Backloggd draw an analogy by saying Backloggd is for games the same way Goodreads is for books. 


The Backloggd dataset merges five datasets - Developers, Games, Genres, Platforms, and Scores - using the game ID column as the key column since each game has a unique seven-digit ID. The resulting comprehensive Backloggd dataset has 12 variables: **id, developer, name, date, rating, reviews, plays, playing, backlogs, wishlists, description, and genre.** As mentioned, **id** represents a unique identification number for each game. Developers or publishers such as Atari or Mattel Electronics are listed under **developer** while the name of the game itself is under **name.** Listed in the YYYY-MM-DD format, the date is recorded under **date.** The next six variables in the list are numerical: **rating** is the average rating of the video game on Backloggd, **reviews** is the number of reviews that contribute to that rating, **plays** is the total number of players which is not to be mistaken for the number of players currently playing under **playing**; then **backlogs** is the number of additions of a video game to the backlog, and **wishlists**  is the number of times a video game has been favorited. Finally, **description** is a full written description of the video game by the publisher, and genres like Sport or Puzzle are listed under **genre.** These 12 variables are all pertinent to the investigation of our questions and are used in the initial exploratory analysis, although the six numerical variables tend to be used more often in later stages of this study. Values are recorded for all of these variables for 398,089 observations, each representing a unique video game listed on Backloggd.

Now equipped with a more thorough understanding of the origin of this dataset along with a breakdown of its variables, it’s time to explore patterns and behaviors of the dataset at-a-glance. Since our questions for further investigation are focused on predicting success by measuring the **rating** variable, a good first step is obtaining the summary statistics for the numerical variables. Some quick, elementary manipulation yields the results in the table below. Counts of observations and counts of NA values are listed under the **Count** and **Missing** columns for each variable, respectively. Summary statistics are provided as either NA or a numeric value in the table for **Mean**, standard deviation (**SD**), **Median**, minimum (**Min**), maximum (**Max**), and **Range.**

```{r, echo=FALSE, message=FALSE, warning=FALSE}
descriptive_table <- data.frame(
  Variables = names(backloggd),
  Data_Type = sapply(backloggd, class),
  Count = sapply(backloggd, function(x) sum(!is.na(x))),
  Missing = sapply(backloggd, function(x) sum(is.na(x))),
  Summary = describe(backloggd)
) %>%
select(-1,-5,-6,-10,-11,-15,-16,-17) %>%
rename(Mean = `Summary.mean`, SD = `Summary.sd`,Median = `Summary.median`,Min = `Summary.min`,Max=`Summary.max`,Range=`Summary.range`)%>%
mutate(Mean = ifelse(Data_Type %in% c("character","Date"),NA,Mean),
       SD = ifelse(Data_Type %in% c("character","Date"),NA,SD),
       Median = ifelse(Data_Type %in% c("character","Date"),NA,Median),
       Min = ifelse(Data_Type %in% c("character","Date"),NA,Min),
       Max = ifelse(Data_Type %in% c("character","Date"),NA,Max),
       Range = ifelse(Data_Type %in% c("character","Date"),NA,Range)
       )

descriptive_table %>% 
          kable(caption = "Descriptive Information on our Backloggd Dataset.") %>% 
          kable_styling(full_width = F, bootstrap_options = c("striped", "bordered", "condensed"))
```


Although **id** is a numeric variable, it is treated as an identifying key rather than a value, so its descriptive statistics are considered negligible. The only thing to note is that the count of **id** is equal to the count of **name** which is 398,089 and intuitively makes sense since each game’s ID should correspond to a single name. This also aligns with the total number of observations as mentioned previously. Next, pay attention to higher counts of missing variables which may be of concern; for example, there are 228,607 missing ratings which will need to be accounted for during the analysis. Otherwise, the **Count** and **Missing** columns are as expected: most observations are accounted for and are recorded in the dataset. Regarding summary statistics, there are NA values recorded for the categorical and date variables as we would expect since these descriptive statistics are more integral in the analysis of numerical variables. Important takeaways include the observation that no game received the lowest possible rating (0) while some games did receive the highest possible rating (5). Also, there are **reviews** for nearly every observation, with the most being a significant 5464, which is much higher than the average of about 14.15. Notably, there are negative values possible for **plays**, **playing**, **backlogs**, and **wishlists.** Another observation made about these summary statistics is that there are significantly more total players (**plays**) compared to **playing** which highlights the difference between total and current players. Similarly, **backlogs** tends to be about twice the size of **wishlists**, indicating more games are in the queue to be played rather than being favorited by a user. In general, this table helps shed light on some patterns, behaviors, and outliers among the variables in the dataset.

Following the descriptive statistics obtained from the table, a visual aid will help further illustrate the patterns in the numeric variables. The following figure contains six density plots of **ratings**, **reviews**, **plays**, **playing**, **backlogs**, and **wishlists** respectively. We see a roughly Normal distribution of **ratings** with an average of about 3, aligning with the mean value from the table. The remaining plots exhibit skewed behavior, with **backlogs**, **wishlists**, and **plays** displaying a strikingly similar distribution, though over different ranges of values that properly contextualize each variable. Also of note is the somewhat oscillatory behavior of the **reviews** and **playing** distributions. Since games are reviewed on a discrete scale, it makes sense that the density drops between these values, yielding the decreasing oscillations apparent in the graph. The behavior of the **playing** plot is most interesting, in that the density seems to oscillate between zero and positive numbers that decrease over time. This reflects that the number of active or current players tends to decrease over time, and the dataframe for **playing** is likely very sparse. Overall, the provided figure achieves the description of numerical variables that we sought, and is a great addition to the previous table in understanding the Backloggd dataset at-a-glance.

```{r,echo=FALSE}
backloggd = na.omit(backloggd)
#ratings 
ratings =  backloggd$rating

Q1_1 = quantile(ratings, 0.25)
Q3_1 = quantile(ratings, 0.75)
IQR_1 = Q3_1 - Q1_1
lower_bound_1 = Q1_1 - 1.5 * IQR_1
upper_bound_1 = Q3_1 + 1.5 * IQR_1
no_outliers_ratings = ratings[ratings >= lower_bound_1 & ratings <= upper_bound_1]

data = data.frame(values = no_outliers_ratings)
plot1 = ggplot(data, aes(x = no_outliers_ratings)) +
  geom_density(fill="red", alpha=0.5) +
  ggtitle("Density Plot of Ratings") +
  xlab("Ratings") +
  ylab("Density")


#reviews 
reviews =  backloggd$reviews

Q1_2 = quantile(reviews, 0.25)
Q3_2 = quantile(reviews, 0.75)
IQR_2 = Q3_2 - Q1_2
lower_bound_2 = Q1_2 - 1.5 * IQR_2
upper_bound_2 = Q3_2 + 1.5 * IQR_2
no_outliers_reviews = reviews[reviews >= lower_bound_2 & reviews <= upper_bound_2]

data2 = data.frame(values = no_outliers_reviews)
plot2 = ggplot(data2, aes(x = no_outliers_reviews)) +
  geom_density(fill="red", alpha=0.5) +
  ggtitle("Density Plot of Reviews") +
  xlab("Reviews") +
  ylab("Density")

#plays 
plays =  backloggd$plays

Q1_3 = quantile(plays, 0.25)
Q3_3 = quantile(plays, 0.75)
IQR_3 = Q3_3 - Q1_3
lower_bound_3 = Q1_3 - 1.5 * IQR_3
upper_bound_3 = Q3_3 + 1.5 * IQR_3
no_outliers_plays = plays[plays >= lower_bound_3 & plays <= upper_bound_3]

data3 = data.frame(values = no_outliers_plays)
plot3 = ggplot(data3, aes(x = no_outliers_plays)) +
  geom_density(fill="red", alpha=0.5) +
  ggtitle("Density Plot of Plays") +
  xlab("Plays") +
  ylab("Density")

#playing
playing =  backloggd$playing

Q1_4 = quantile(playing, 0.25)
Q3_4 = quantile(playing, 0.75)
IQR_4 = Q3_4 - Q1_4
lower_bound_4 = Q1_4 - 1.5 * IQR_4
upper_bound_4 = Q3_4 + 1.5 * IQR_4
no_outliers_playing = playing[playing >= lower_bound_4 & playing <= upper_bound_4]

data4 = data.frame(values = no_outliers_playing)
plot4 = ggplot(data4, aes(x = no_outliers_playing)) +
  geom_density(fill="red", alpha=0.5) +
  ggtitle("Density Plot of Playing") +
  xlab("Playing") +
  ylab("Density")


#backlogs 
backlogs =  backloggd$backlogs

Q1_5 = quantile(backlogs, 0.25)
Q3_5 = quantile(backlogs, 0.75)
IQR_5 = Q3_5 - Q1_5
lower_bound_5 = Q1_5 - 1.5 * IQR_5
upper_bound_5 = Q3_5 + 1.5 * IQR_5
no_outliers_backlogs = backlogs[backlogs >= lower_bound_5 & backlogs <= upper_bound_5]

data5 = data.frame(values = no_outliers_backlogs)
plot5 = ggplot(data5, aes(x = no_outliers_backlogs)) +
  geom_density(fill="red", alpha=0.5) +
  ggtitle("Density Plot of Backlogs") +
  xlab("Backlogs") +
  ylab("Density")

#wishlists
wishlists =  backloggd$wishlists

Q1_6 = quantile(wishlists, 0.25)
Q3_6 = quantile(wishlists, 0.75)
IQR_6 = Q3_6 - Q1_6
lower_bound_6 = Q1_6 - 1.5 * IQR_6
upper_bound_6 = Q3_6 + 1.5 * IQR_6
no_outliers_wishlists = wishlists[wishlists >= lower_bound_6 & wishlists <= upper_bound_6]

data6 = data.frame(values = no_outliers_wishlists)
plot6 = ggplot(data6, aes(x = no_outliers_wishlists)) +
  geom_density(fill="red", alpha=0.5) +
  ggtitle("Density Plot of Wishlists") +
  xlab("Wishlists") +
  ylab("Density")

```


```{r, echo=FALSE, message=FALSE}
library(gridExtra)

grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, ncol=3, nrow=2)

```

This rigorous breakdown of the dataset allows for a clearer starting point for our predictive models. Understanding general trends, what’s missing, and what can be quantified will enable us to launch immediately into our investigation with a strong set of predictors. The next steps include model building and measuring accuracy so we can answer our initial questions.


# RESULTS

To answer our first question, "Could we predict the success of a game without knowing the publisher?", we utilized k-fold cross-validation and Mean Squared Error (MSE) calculations. This technique divides the proposed data into 'k' parts. A percentage of each part is considered the test set while the rest is the train set. It is repeated for 'k' times and Mean Squared Error (MSE) or Mean Absolute Error (MAE) is estimated. As an initial step, we divide our data using random sampling into training and testing sets, with the former accounting for 80% of the data and the latter 20%. Then, we created full and empty models to compare the MSE to the final model. It is important to carefully consider which variables you are utilizing in a regression model. If two or more variables are highly correlated to one another, multicollinearity can occur. This could result in less reliable statistical inferences, hence why we removed variables/column names **name** and **id**. We removed **developer** because we are attempting to predict a game's success without prior knowledge of the publisher, and we removed **description** as it is not valuable for the analysis in this case.

Our use of multiple linear regression interprets the interactions of multifaceted relationships. We relied on Mean Squared Error (MSE) due to its fast convergence, particularly when errors are consistently small. In Model 1, we calculated the MAE (Mean Absolute Error) of the differences between the predicted values of our full and empty model and the actual values of the ratings. These values were significantly larger than the values given by the MSE, hence our reason for continuing our analysis with continued use of loss function MSE.

To continue with regression, we fit two linear models, one called **modelfull** which takes an R formula with rating as the outcome variable and **date+reviews+plays+playing+backlogs+wishlists+genre** as the collection of predictor variables. Similarly, we created **modelnone** with rating as the outcome variable predicted by no other variable (so the natural prediction is the mean of rating itself). It is important to emphasize our use of the function na.omit() to remove all observations with missing data on any variable in the dataset.

We followed this with full and empty model predictions on our test data. The MSE, denoted **mse1**, is the average of the squared differences between the predicted values of our full model and the actual values of the ratings. The number given is 0.465 which we can compare to .505, the average of the squared differences between the predicted values of our empty model and the actual values of the ratings. Note that a higher MSE implies that the data points are spread more widely around their central tendency (mean), while a lower MSE indicates the opposite.

We then used 10-fold cross-validation along with Elastic Net regression to traverse through alpha values from 0 to 1 in increments of 0.1. We used Elastic Net to address regression limitations by introducing two hyperparameters, alpha and lambda, to control the amount of regularization applied. It is particularly useful when dealing with datasets with high dimensionality and a large number of predictors. The cross-validation MSE is then calculated and the best lambda is selected for each alpha. For example, the alpha value of 0.0 has a lambda value of 0.144 and an MSE value of .476. With this list of values, we found the top four best models based on MSE. We have a table that outputs the best four models created through the 10 different alphas, with the best lambda values and MSE. The best model, which we have concluded to be alpha = 0.2 as shown in the table, is plotted below the table.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(123)
index <- sample(1:nrow(backloggd), size = 0.8 * nrow(backloggd))

train_data <- backloggd[index, ]
test_data <- backloggd[-index, ]

test_data = na.omit(test_data)
train_data <- na.omit(train_data)


RESULT=NULL
for (i in 0:10) {
    cv.out = cv.glmnet(x=as.matrix(train_data[, -c(1:3, 5, 11)]),
                 y=as.vector(train_data[, 5]),
                       type.measure="mse", 
                       alpha=i/10)
    alpha=i/10
    best.lambda=cv.out$lambda.1se
    y.test=predict(cv.out,s=best.lambda,newx=as.matrix(test_data[, -c(1:3, 5, 11)]))
    out.mse=mean((test_data$rating-y.test)^2)
    RESULT=rbind(RESULT,c(alpha,best.lambda,out.mse))
}
colnames(RESULT)=c("alpha","lambda","MSE")

RESULT2=as.data.frame(RESULT) %>% filter(rank(MSE)<=4)
```
```{r, echo=FALSE, message=FALSE, warning=FALSE}

RESULT2 %>% 
          kable(caption = "Top Four Models") %>% 
          kable_styling(full_width = F, bootstrap_options = c("striped", "bordered", "condensed"))
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}


enet.mod=glmnet(x=as.matrix(train_data[, -c(1:3, 5, 11)]),
                 y=as.vector(train_data[, 5]),
                 alpha=0.2)

par(mar = c(5, 5, 5, 5))
plot(
  enet.mod,
  xvar = "lambda", 
    main = "Plot of glmnet Model
  
  "
)

```


To answer our second question, "Can we come up with creative metrics on the publisher to help us predict the rating of a game more accurately?", we utilized the top-performing model mentioned earlier and incorporated additional publisher metrics. These metrics include the standard deviation of ratings for a specific publisher, the quantity of games released by that publisher, and the rating of their most recent release.

The **backloggd** dataset initially contained a variable named **date** which is the release date of a specific video game. However, to enhance its suitability for our regression analysis, we converted this variable to **months_old**, which is a video game's age measured in months. The analysis is restricted to data up to December 31, 2023. Additionally, we removed all duplicated **ids** in the **backloggd** data frame and used na.omit() to handle missing values. These steps tidy our data for a smoother, more structured analysis.

The full model predictions in the above question had a lower MSE compared to the empty model predictions, therefore, we used the full model methodology with **months_old** replacing date. The MSE value from **modelfull** with **months_old** is 0.497 compared to the value of 0.465 from the full model in the first question. A small increase like this may be insignificant, but it suggests that the model's accuracy has slightly decreased, and it's making slightly less precise predictions. Compared to the MSE value of .505, a small decrease in MSE generally indicates an improvement in the model's performance.

To investigate a noteworthy interaction between specific creative metrics and publishers, aiming to enhance the accuracy of predicting a game's rating, we determined the MSE value using the actual values of ratings and the linear model above along with an isolated interaction term that multiplies the number of games produced by each developer by the number of games in each genre factor. The MSE value produced was .495, which is a slight improvement from .497.

Our next idea for additional publisher metrics would be the standard deviation of ratings. By including the standard deviation of the sample ratings per developer as one of the predictor variables used to fit a linear model, we found that the MSE value produced is .482. Similarly, we used the average rating of games by a certain developer. Using identical methods, we found that the MSE value of the average rating of games in a publisher's catalog as an added predictor variable is .284. This decrease suggests that this model's accuracy has increased, making it a viable metric for predicting the rating of a video game more accurately.

We also used the average number of active players by publisher, the interaction between the standard deviation of ratings and a game's age, as well as the rating of the most recently released game as additional predictor variables for predicting a game's rating. The MSE values across these models are depicted below. We can also see residual vs. fitted plots below the MSE values, which help our analysis and understanding of how well the models really fit.



```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(lubridate)

backloggd = backloggd %>%
  mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
  distinct(id, .keep_all = TRUE)%>% #Removes Duplicate ids
  filter(date <= as.Date("2023-12-31")) %>%             
  na.omit() %>%                                         
  mutate(months_old = interval(date, as.Date("2024-01-01")) %/% months(1)) %>%
  arrange(desc(date)) %>%                               
  group_by(date)                                        

set.seed(123)
index <- sample(1:nrow(backloggd), size = 0.8 * nrow(backloggd))
train_data <- backloggd[index, ]
test_data <- backloggd[-index, ]
modelfull = lm(rating~months_old+reviews+plays+playing+backlogs+wishlists+genre, data=train_data)

predictions1 <- predict(modelfull, newdata = test_data)
actuals = test_data$rating
# Calculate Mean Squared Error (MSE)
mse1 = mean((predictions1-actuals)^2)


num_games_by_developer = backloggd %>%
  group_by(developer) %>%
  summarise(num_games = n(), .groups = 'drop')  

backloggd2 = backloggd %>%
  left_join(num_games_by_developer, by = "developer")


index2 = sample(1:nrow(backloggd2), size = 0.8 * nrow(backloggd2))
train_data2 = backloggd2[index2, ]
test_data2 = backloggd2[-index2, ]

model_1 = lm(rating ~ months_old + reviews + plays + playing + 
            backlogs + wishlists + genre + I(as.numeric(as.factor(genre)) * num_games), 
            data = train_data2)

predictions_1 = predict(model_1, newdata = test_data2)
actuals_1 = test_data2$rating
# Calculate Mean Squared Error (MSE)
mse_1 = mean((predictions_1-actuals_1)^2)

developer_std_dev = backloggd2 %>%
  group_by(developer) %>%
  summarise(std_dev_rating = sd(rating, na.rm = TRUE))

backloggd3 = backloggd2 %>%
  left_join(developer_std_dev, by = "developer") %>%
  na.omit()

index3 = sample(1:nrow(backloggd3), size = 0.8 * nrow(backloggd3))
train_data3 = backloggd3[index3, ]
test_data3 = backloggd3[-index3, ]

model_2 = lm(formula = rating ~ months_old + reviews + plays + playing + 
    backlogs + wishlists + genre + std_dev_rating, 
    data = train_data3)


#this variable is significant, now test mse 

predictions_2 = predict(model_2, newdata = test_data3)
actuals_2 = test_data3$rating
# Calculate Mean Squared Error (MSE)
mse_2 = mean((predictions_2-actuals_2)^2)
#next idea: interaction between std of ratings and age of game

library(car)

avg_dev_rating <- backloggd3 %>%
  group_by(developer)%>%
  summarise(avg_dev_rating = mean(rating, na.rm = TRUE))
backloggd4 <- backloggd3 %>%
      left_join(avg_dev_rating, by = "developer") %>%
      na.omit()

index4 = sample(1:nrow(backloggd4), size = 0.8 * nrow(backloggd4))
train_data4 = backloggd4[index4, ]
test_data4 = backloggd4[-index4, ]

model_3 = lm(formula = rating ~ months_old + reviews + plays + playing + 
    backlogs + wishlists + genre + I(as.numeric(as.factor(genre)) * num_games)+avg_dev_rating, 
    data = train_data4)


predictions_3 = predict(model_3, newdata = test_data4)
actuals_3 = test_data4$rating
# Calculate Mean Squared Error (MSE)
mse_3 = mean((predictions_3-actuals_3)^2)

avg_num_active <- backloggd4 %>%
  group_by(developer)%>%
  summarise(
    n = n(),
    avg_num_active = mean(playing)
  )

backloggd5 <- backloggd4 %>%
      left_join(avg_num_active, by = "developer") %>%
      na.omit()

index5 = sample(1:nrow(backloggd5), size = 0.8 * nrow(backloggd5))
train_data5 = backloggd5[index5, ]
test_data5 = backloggd5[-index5, ]

model_4 = lm(formula = rating ~ months_old + reviews + plays + playing + 
    backlogs + wishlists + genre + I(as.numeric(as.factor(genre)) * num_games)+avg_dev_rating + avg_num_active, 
    data = train_data5)


predictions_4 = predict(model_4, newdata = test_data5)
actuals_4 = test_data5$rating
# Calculate Mean Squared Error (MSE)
mse_4 = mean((predictions_4-actuals_4)^2)

model_5 = lm(formula = rating ~ months_old + reviews + plays + playing + 
    backlogs + wishlists + genre + I(as.numeric(as.factor(genre)) * num_games)+avg_dev_rating + avg_num_active + I(std_dev_rating * months_old), 
    data = train_data5)

predictions_5 = predict(model_5, newdata = test_data5)
actuals_5 = test_data5$rating
mse_5 = mean((predictions_5-actuals_5)^2)


#next idea: rating of the last game 

backloggd6 = backloggd5 %>%
  arrange(developer, desc(date))  

most_recent_ratings = backloggd6 %>%
  group_by(developer) %>%
  slice(1) %>%
  ungroup() %>%
  select(developer, most_recent_rating = rating) 

backloggd6 = backloggd5 %>%
  left_join(most_recent_ratings, by = "developer")


index6 = sample(1:nrow(backloggd6), size = 0.8 * nrow(backloggd6))
train_data6 = backloggd6[index6, ]
test_data6 = backloggd6[-index6, ]

model_6 = lm(formula = rating ~ months_old + reviews + plays + playing + 
    backlogs + wishlists + genre + I(as.numeric(as.factor(genre)) * num_games)+avg_dev_rating + avg_num_active + I(std_dev_rating * months_old) + most_recent_rating,
    data = train_data6)


predictions_6 = predict(model_6, newdata = test_data6)
actuals_6 = test_data6$rating
mse_6 = mean((predictions_6-actuals_6)^2)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)

mse_data = data.frame(
  Model = c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5", "Model 6"),
  MSE = c(mse_1, mse_2, mse_3, mse_4, mse_5, mse_6))

ggplot(mse_data, aes(x = Model, y = MSE, fill = Model)) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "How MSE Compares Across Models",
       x = "Model",
       y = "MSE") 
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow = c(2, 3))
modss_as_list = list(model_1, model_2,model_3, model_4, model_5, model_6)

for (i in 1:length(modss_as_list)) {
  residuals = residuals(modss_as_list[[i]])
  fittedvalues = fitted(modss_as_list[[i]])
  titlesforplots = sprintf("Residual vs. Fitted for Model %d", i)
  plot(fittedvalues, residuals, xlab = "Fitted values", ylab = "Residuals",
       main = titlesforplots)
  abline(h = 0, col = "green")
  }
```

# CONCLUSION

The answer to our question, "Could we predict the success of a game without knowing the publisher?" is "Yes." The answer to "Can we come up with creative metrics on the publisher to help us predict the rating of the game more accurately?" is "Yes." By employing a multiple linear regression model, we were able to predict the ratings of a game based on distinct predictor variables that do not include publisher information and are beneficial to our analysis. Additionally, we found that there are four metrics based on a game's publisher that better predict the rating of the game more accurately. The metrics are the average rating of games in a publisher's catalog, the average number of active players per publisher, the interaction between the standard deviation of ratings and how many months have passed since a game's release date, and finally, the rating of a publisher's most recently released game. The MSE values of these four metrics are below 0.3 whereas the other metrics are above .450. 

The video game industry is rapidly evolving, contributing billions of dollars to the United States economy annually. Video game ratings strongly influence consumer perception and purchase decisions, often leading to higher profits for developers and publishers. These ratings serve as a valuable guide for customers, developers, and publishers alike.

Amidst a vast array of video games competing for consumer attention, analyzing factors beyond the publisher can provide insights into a game's success. For developers and publishers, such analysis can inform the production of games with higher ratings. Further examination of the correlation between profit and rating could enhance our understanding. Incorporating revenue data alongside video game ratings can facilitate a more thorough examination of a game's success. Additional metrics such as social media engagement and Metacritic scores offer a broader understanding of video game performance, player preferences, and market dynamics. Lower-rated games can provide valuable feedback for improvement.

Despite volatility stemming from factors like seasonality, competition, and technological advancements, the video game industry offers opportunities for growth and improvement. Players, publishers, and developers must adapt to change and manage risks effectively to navigate the landscape successfully.







